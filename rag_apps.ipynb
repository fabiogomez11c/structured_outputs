{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "client = instructor.patch(OpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Improving extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extraction(BaseModel):\n",
    "    topic: str\n",
    "    summary: str\n",
    "    hypothetical_questions: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Hypothetical questions that this document could answer\",\n",
    "    )\n",
    "    keywords: List[str] = Field(\n",
    "        default_factory=list, description=\"Keywords that this document is about\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hypothetical_questions': ['What are the key features of a simple RAG system?',\n",
      "                            'How does a simple RAG system perform embedding '\n",
      "                            'searches?',\n",
      "                            'Why might a simple RAG system fail to provide '\n",
      "                            'accurate results for complex queries?'],\n",
      " 'keywords': ['simple RAG',\n",
      "              'Retrieval-Augmented Generation',\n",
      "              'vector database',\n",
      "              'embedding search',\n",
      "              'complex queries',\n",
      "              'limitations'],\n",
      " 'summary': 'The simplest implementation of Retrieval-Augmented Generation '\n",
      "            '(RAG) involves embedding a user query and performing a single '\n",
      "            'embedding search in a vector database, which often struggles with '\n",
      "            'complex queries and diverse data sources due to inherent '\n",
      "            'limitations.',\n",
      " 'topic': 'Simple RAG'}\n",
      "{'hypothetical_questions': ['What causes query-document mismatch in RAG '\n",
      "                            'systems?',\n",
      "                            'How can query-document mismatch impact the '\n",
      "                            'effectiveness of search results?',\n",
      "                            'Can the issue of query-document mismatch in RAG '\n",
      "                            'systems be mitigated, and if so, how?'],\n",
      " 'keywords': ['Query-Document Mismatch',\n",
      "              'embedding alignment',\n",
      "              'irrelevant information',\n",
      "              'complex queries'],\n",
      " 'summary': 'Simple RAG systems often face a mismatch between the query and '\n",
      "            'document embeddings, leading to retrieval of irrelevant '\n",
      "            'information when the specific intersection of topics in a complex '\n",
      "            'query is not matched in vector space.',\n",
      " 'topic': 'Query-Document Mismatch'}\n",
      "{'hypothetical_questions': ['What is a monolithic search backend in RAG '\n",
      "                            'systems?',\n",
      "                            'Why is depending on a single search method a '\n",
      "                            'limitation for RAG systems?',\n",
      "                            'How can RAG systems improve to handle multiple '\n",
      "                            'data sources?'],\n",
      " 'keywords': ['Monolithic Search Backend',\n",
      "              'flexibility',\n",
      "              'multiple data sources',\n",
      "              'RAG system limitations'],\n",
      " 'summary': 'Simple RAG systems depend on a monolithic search backend, leading '\n",
      "            'to reduced flexibility and inability to leverage multiple and '\n",
      "            'varied data sources effectively.',\n",
      " 'topic': 'Monolithic Search Backend'}\n",
      "{'hypothetical_questions': ['What are the text search limitations in RAG '\n",
      "                            'systems?',\n",
      "                            \"Why can't simple RAG systems handle nuanced \"\n",
      "                            'search queries?',\n",
      "                            'What kind of complex search features are beyond '\n",
      "                            'the capability of simple RAG?'],\n",
      " 'keywords': ['Text Search Limitations',\n",
      "              'simple text queries',\n",
      "              'complex search features',\n",
      "              'nuanced queries'],\n",
      " 'summary': 'Simple RAG systems are limited to straightforward text queries '\n",
      "            'and unable to process the nuances of more complex search '\n",
      "            'features, leading to potential difficulties in retrieving '\n",
      "            'specific information.',\n",
      " 'topic': 'Text Search Limitations'}\n",
      "{'hypothetical_questions': ['How does limited planning ability affect RAG '\n",
      "                            'system performance?',\n",
      "                            'Why is it important for RAG systems to consider '\n",
      "                            'contextual information?',\n",
      "                            'What strategies could enhance the planning '\n",
      "                            'ability of RAG systems?'],\n",
      " 'keywords': ['Limited Planning Ability',\n",
      "              'contextual information',\n",
      "              'RAG systems',\n",
      "              'search relevance'],\n",
      " 'summary': 'Simple RAG systems lack the capacity to incorporate additional '\n",
      "            'contextual information, which could be critical in refining and '\n",
      "            'improving the relevance of search results.',\n",
      " 'topic': 'Limited Planning Ability'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "text_chunk = \"\"\"\n",
    "## Simple RAG\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n",
    "\n",
    "**What are the limitations?**\n",
    "\n",
    "- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n",
    "    - Query: \"Tell me about climate change effects on marine life.\"\n",
    "    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n",
    "- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n",
    "    - Query: \"Latest research in quantum computing.\"\n",
    "    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n",
    "- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n",
    "    - Query: \"what problems did we fix last week\"\n",
    "    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n",
    "- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n",
    "    - Query: \"Tips for first-time Europe travelers.\"\n",
    "    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n",
    "\"\"\"\n",
    "\n",
    "extractions = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    stream=True,\n",
    "    response_model=Iterable[Extraction],\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "for extraction in extractions:\n",
    "    pprint(extraction.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Understanding 'recent queries' to add temporal context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    start: date\n",
    "    end: date\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    rewritten_query: str\n",
    "    published_daterange: DateRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query(rewritten_query='Recent developments in AI', published_daterange=DateRange(start=datetime.date(2024, 1, 1), end=datetime.date(2024, 3, 27)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_query(q) -> Query:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        response_model=Query,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "query = expand_query(\"What are some recent developments in AI?\")\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query(rewritten_query='latest advancements in artificial intelligence technology', published_daterange=DateRange(chain_of_thought='To find the most recent developments in AI, a search starting from one year ago to the present would be a good range to capture the latest advancements.', start=datetime.date(2023, 3, 27), end=datetime.date(2024, 3, 27)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DateRange(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Think step by step to plan what is the best time range to search in\"\n",
    "    )\n",
    "    start: date\n",
    "    end: date\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    rewritten_query: str = Field(\n",
    "        description=\"Rewrite the query to make it more specific\"\n",
    "    )\n",
    "    published_daterange: DateRange = Field(\n",
    "        description=\"Effective date range to search in\"\n",
    "    )\n",
    "\n",
    "\n",
    "def expand_query(q) -> Query:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        response_model=Query,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "expand_query(\"What are some recent developments in AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import instructor\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from helpers import dicts_to_df\n",
    "from datetime import date\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Think step by step to plan what is the best time range to search in\"\n",
    "    )\n",
    "    start: date\n",
    "    end: date\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    rewritten_query: str = Field(\n",
    "        description=\"Rewrite the query to make it more specific\"\n",
    "    )\n",
    "    published_daterange: DateRange = Field(\n",
    "        description=\"Effective date range to search in\"\n",
    "    )\n",
    "\n",
    "    def report(self):\n",
    "        dct = self.model_dump()\n",
    "        dct[\"usage\"] = self._raw_response.usage.model_dump()\n",
    "        return dct\n",
    "\n",
    "\n",
    "\n",
    "# We'll use a different client for async calls\n",
    "# To highlight the difference and how we can use both\n",
    "aclient = instructor.patch(AsyncOpenAI())\n",
    "\n",
    "\n",
    "async def expand_query(\n",
    "    q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0\n",
    ") -> Query:\n",
    "    return await aclient.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temp,\n",
    "        response_model=Query,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabiogomez11c\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fh/code/structured_outputs/wandb/run-20240327_114805-lzk5t4nt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabiogomez11c/structured_outptus/runs/lzk5t4nt/workspace' target=\"_blank\">pretty-planet-1</a></strong> to <a href='https://wandb.ai/fabiogomez11c/structured_outptus' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabiogomez11c/structured_outptus' target=\"_blank\">https://wandb.ai/fabiogomez11c/structured_outptus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabiogomez11c/structured_outptus/runs/lzk5t4nt/workspace' target=\"_blank\">https://wandb.ai/fabiogomez11c/structured_outptus/runs/lzk5t4nt/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average duration (s)</td><td>▁</td></tr><tr><td>duration (s)</td><td>▁</td></tr><tr><td>n_queries</td><td>▁</td></tr><tr><td>usage_completion_tokens</td><td>▁</td></tr><tr><td>usage_prompt_tokens</td><td>▁</td></tr><tr><td>usage_total_tokens</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average duration (s)</td><td>1.86971</td></tr><tr><td>duration (s)</td><td>7.47882</td></tr><tr><td>n_queries</td><td>4</td></tr><tr><td>usage_completion_tokens</td><td>410</td></tr><tr><td>usage_prompt_tokens</td><td>780</td></tr><tr><td>usage_total_tokens</td><td>1190</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-planet-1</strong> at: <a href='https://wandb.ai/fabiogomez11c/structured_outptus/runs/lzk5t4nt/workspace' target=\"_blank\">https://wandb.ai/fabiogomez11c/structured_outptus/runs/lzk5t4nt/workspace</a><br/>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240327_114805-lzk5t4nt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "model = \"gpt-4-1106-preview\"\n",
    "temp = 0\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"structured_outptus\",\n",
    "    config={\"model\": model, \"temp\": temp},\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"latest developments in artificial intelligence last 3 weeks\",\n",
    "    \"renewable energy trends past month\",\n",
    "    \"quantum computing advancements last 2 months\",\n",
    "    \"biotechnology updates last 10 days\",\n",
    "]\n",
    "start = time.perf_counter()\n",
    "queries = await asyncio.gather(\n",
    "    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n",
    ")\n",
    "duration = time.perf_counter() - start\n",
    "\n",
    "with open(\"schema.json\", \"w+\") as f:\n",
    "    schema = Query.model_json_schema()\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "with open(\"results.jsonlines\", \"w+\") as f:\n",
    "    for query in queries:\n",
    "        f.write(query.model_dump_json() + \"\\n\")\n",
    "\n",
    "df = dicts_to_df([q.report() for q in queries])\n",
    "df[\"input\"] = test_queries\n",
    "df.to_csv(\"results.csv\")\n",
    "\n",
    "\n",
    "run.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\n",
    "\n",
    "run.log(\n",
    "    {\n",
    "        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n",
    "        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n",
    "        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n",
    "        \"duration (s)\": duration,\n",
    "        \"average duration (s)\": duration / len(queries),\n",
    "        \"n_queries\": len(queries),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "run.log(\n",
    "    {\n",
    "        \"results\": wandb.Table(dataframe=df),\n",
    "    }\n",
    ")\n",
    "\n",
    "files = wandb.Artifact(\"data\", type=\"dataset\")\n",
    "\n",
    "files.add_file(\"schema.json\")\n",
    "files.add_file(\"results.jsonlines\")\n",
    "files.add_file(\"results.csv\")\n",
    "\n",
    "\n",
    "run.log_artifact(files)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Personal Assistants, parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class SearchClient(BaseModel):\n",
    "    query: str = Field(description=\"The search query that will go into the search bar\")\n",
    "    keywords: List[str]\n",
    "    email: str\n",
    "    source: Literal[\"gmail\", \"calendar\"]\n",
    "    date_range: DateRange\n",
    "\n",
    "\n",
    "class Retrival(BaseModel):\n",
    "    queries: List[SearchClient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"work\",\n",
      "            \"keywords\": [\n",
      "                \"work\",\n",
      "                \"today\"\n",
      "            ],\n",
      "            \"email\": \"jason@work.com\",\n",
      "            \"source\": \"gmail\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"Check for work-related emails for today\",\n",
      "                \"start\": \"2024-03-27\",\n",
      "                \"end\": \"2024-03-27\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"new emails\",\n",
      "            \"keywords\": [\n",
      "                \"new\",\n",
      "                \"emails\"\n",
      "            ],\n",
      "            \"email\": \"jason@work.com\",\n",
      "            \"source\": \"gmail\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"Check for any new emails received today\",\n",
      "                \"start\": \"2024-03-27\",\n",
      "                \"end\": \"2024-03-27\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrival = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=Retrival,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are Jason's personal assistant.\n",
    "                He has two emails jason@work.com jason@personal.com \n",
    "                Today is {date.today()}\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\n",
    "    ],\n",
    ")\n",
    "print(retrival.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"Today's meetings\",\n",
      "            \"keywords\": [\n",
      "                \"meeting\",\n",
      "                \"appointment\",\n",
      "                \"calendar event\"\n",
      "            ],\n",
      "            \"email\": \"jason@work.com\",\n",
      "            \"source\": \"calendar\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"As today is 2024-03-27, I should search for calendar events on this specific date.\",\n",
      "                \"start\": \"2024-03-27\",\n",
      "                \"end\": \"2024-03-27\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Important emails\",\n",
      "            \"keywords\": [\n",
      "                \"important\",\n",
      "                \"urgent\",\n",
      "                \"priority\"\n",
      "            ],\n",
      "            \"email\": \"jason@personal.com\",\n",
      "            \"source\": \"gmail\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"Since I'm asked for important emails, they are often marked as important or urgent recently so I will search for the last week.\",\n",
      "                \"start\": \"2024-03-20\",\n",
      "                \"end\": \"2024-03-27\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrival = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    response_model=Retrival,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are Jason's personal assistant.\n",
    "                He has two emails jason@work.com jason@personal.com \n",
    "                Today is {date.today()}\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(retrival.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Decomposing questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"root_question\": \"What is the difference between the population of Jason's home country and Canada?\",\n",
      "    \"plan\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"query\": \"What is Jason's home country?\",\n",
      "            \"subquestions\": []\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"query\": \"What is the population of Jason's home country?\",\n",
      "            \"subquestions\": [\n",
      "                1\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"query\": \"What is the population of Canada?\",\n",
      "            \"subquestions\": []\n",
      "        },\n",
      "        {\n",
      "            \"id\": 4,\n",
      "            \"query\": \"What is the difference between the population of two countries?\",\n",
      "            \"subquestions\": [\n",
      "                2,\n",
      "                3\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class Question(BaseModel):\n",
    "    id: int = Field(..., description=\"A unique identifier for the question\")\n",
    "    query: str = Field(..., description=\"The question decomposited as much as possible\")\n",
    "    subquestions: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"The subquestions that this question is composed of\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    root_question: str = Field(..., description=\"The root question that the user asked\")\n",
    "    plan: List[Question] = Field(\n",
    "        ..., description=\"The plan to answer the root question and its subquestions\"\n",
    "    )\n",
    "\n",
    "\n",
    "retrival = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    response_model=QueryPlan,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the difference between the population of jason's home country and canada?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(retrival.model_dump_json(indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soutputs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
